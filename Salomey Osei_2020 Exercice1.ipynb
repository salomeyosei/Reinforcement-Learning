{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Salomey Osei_2020 Exercice1.ipynb","provenance":[{"file_id":"1TfRPoWlk3wVKvxBGsZsdcM6q9266Unuy","timestamp":1582228877309},{"file_id":"1EMpA7lM7ayefTKuvagMlO5Pq9CmUfo0j","timestamp":1582061851831},{"file_id":"1BcYacIbuZXAl0yjVFvy7BpHb7cwvkfjp","timestamp":1582025138205},{"file_id":"https://github.com/fstrub95/RL/blob/master/AIMS_2020.ipynb","timestamp":1581976129836}],"private_outputs":true,"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QYW2YAMZOX4-","colab_type":"text"},"source":["# Reinforcement Learning in Finite MDPs"]},{"cell_type":"markdown","metadata":{"id":"AFu6XLHrtIa6","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"8K4vLMPTSxmn","colab_type":"code","colab":{}},"source":["!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n","!cd mvarl_hands_on && git pull"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NNsuKZxeOX4_","colab_type":"text"},"source":["## MDPs"]},{"cell_type":"code","metadata":{"id":"bNlEnGsYOX5A","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.insert(0, './mvarl_hands_on/utils')\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","import math\n","from gridworld import GridWorldWithPits"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"puk7xD1EBDEu","colab_type":"text"},"source":["## Define the environment"]},{"cell_type":"code","metadata":{"id":"ezz31WR2uJFX","colab_type":"code","colab":{}},"source":["from finite_env import FiniteEnv\n","import numpy as np\n","\n","# Note: You do not need to read this Class. It only redefines the clearning robots\n","\n","\n","class RobotEnv(FiniteEnv):\n","    \"\"\"\n","    Enviroment with 2 states and 3 actions\n","    Args:\n","        gamma (float): discount factor\n","        seed    (int): Random number generator seed\n","    \"\"\"\n","\n","    def __init__(self, gamma=0.5, seed=42):\n","        # Set seed\n","        self.RS = np.random.RandomState(seed)\n","\n","        # Transition probabilities\n","        # shape (Ns, Na, Ns)\n","        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n","\n","        Ns = 2\n","        Na = 3\n","        \n","        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n","        P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n","        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n","\n","        self.state_decoder  = {0: \"A\", 1: \"B\"}\n","        self.action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n","        \n","        # Initialize base class\n","        states = np.arange(Ns).tolist()\n","        action_sets = [np.arange(Na).tolist()]*Ns\n","        super().__init__(states, action_sets, P, gamma)\n","\n","    def reward_func(self, state, action, *_):\n","        return self._R[state, action]\n","\n","    def reset(self, s=0):\n","        self.state = s\n","        return self.state\n","\n","    def step(self, action):\n","        next_state = self.sample_transition(self.state, action)\n","        reward = self.reward_func(self.state, action, next_state)\n","        done = False\n","        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n","            self.state_decoder[state],\n","            self.action_decoder[action],\n","            self.state_decoder[next_state],\n","            reward )}\n","        self.state = next_state\n","\n","        observation = next_state\n","        return observation, reward, done, info\n","\n","    def sample_transition(self, s, a):\n","        prob = self.P[s,a,:]\n","        s_ = self.RS.choice(self.states, p = prob)\n","        return s_\n","\n","    def render_policy(self, policy):\n","      if len(np.array(policy).shape) > 1:\n","        policy = densify_policy(policy)\n","\n","      txt = \"\"\n","      for i, a in enumerate(policy):\n","        txt += \"In state {} perform {}\\n\".format(self.state_decoder[i], self.action_decoder[a])\n","      return txt[:-1]\n","\n","    @property\n","    def R(self):\n","        return self._R\n","  \n","env = RobotEnv()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hdzoq4dLOX5C","colab_type":"text"},"source":["Setting up the environment"]},{"cell_type":"code","metadata":{"id":"WVHOHJJeOX5C","colab_type":"code","colab":{}},"source":["# Useful attributes\n","print(\"Set of states:\", env.states)\n","print(\"Set of actions:\", env.actions)\n","print(\"Number of states: \", env.Ns)\n","print(\"Number of actions: \", env.Na)\n","print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n","print(\"R has shape: \", env.R.shape)  \n","print(\"discount factor: \", env.gamma)\n","print(\"\")\n","\n","# Usefult methods\n","state = env.reset() # get initial state\n","print(\"initial state: \", state)\n","print(\"reward at (s=0, a=1): \", env.reward_func(0,1))\n","print(\"\")\n","\n","# A random policy\n","policy = np.random.randint(env.Na, size = (env.Ns,))\n","print(\"random policy = \", policy)\n","print(env.render_policy(policy))\n","\n","# Interacting with the environment\n","print(\"(s, a, s', r):\")\n","for time in range(4):\n","    action = policy[state]\n","    next_state, reward, done, info = env.step(action)\n","    print(state, action, next_state, reward, \"   --> \" + info[\"str\"] if \"str\" in info else \"\") \n","    if done:\n","        break\n","    state = next_state\n","print(\"\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YM7lLbRA68A","colab_type":"text"},"source":["## Useful functions\n"]},{"cell_type":"code","metadata":{"id":"sekFIFmm6V71","colab_type":"code","colab":{}},"source":["# This functions ains at alternating between sparse and dense policy:\n","# sparse policy allows to perform matrix completion pi \\in R^{SxA}. It is denoted pi in the code Ex: [[1, 0, 0], [0, 1, 0]])\n","# dense policy is a determinist policy whose value are the indexed actions. It is the argmax of pi. It is denoted dpi in the code. Ex [0, 1] \n","\n","def sparsify_policy(policy, Na):\n","  ### Turn a dense policy into a sparse one.\n","  #  Ex: [0, 1], Na=3  -> [[1, 0, 0], [0, 1, 0]]\n","  ###\n","\n","  Ns = len(policy)\n","  sparse_policy = np.zeros(shape=(Ns, Na))\n","  for i, a in enumerate(policy):\n","    sparse_policy[i,a]=1\n","  return sparse_policy\n","\n","def densify_policy(policy):\n","  ### Turn a sparse determinist policy into a dense one.\n","  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n","  ###\n","  return np.array(policy).argmax(axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dyTP0kkAXKo","colab_type":"text"},"source":["## Exercice : Policy Evaluation\n","1. Evaluate the policy by solving the linear system\n","2. Evaluate the policy through recursion"]},{"cell_type":"code","metadata":{"id":"1T0QX3tA2RRU","colab_type":"code","colab":{}},"source":["# Policy evaluation (exact)\n","\n","# Retrieve the environment MDP\n","P = env.P\n","R = env.R\n","gamma = env.gamma\n","\n","\n","# Policy to evaluate\n","# State A: Search\n","# State B: Wait\n","dpi = np.array([1, 0])\n","pi = sparsify_policy(dpi, Na=env.Na)\n","print(\"## old pi:\")\n","print(pi)\n","print(env.render_policy(pi))\n","\n","# states = [0, 1]\n","# actions = [0, 1, 2]\n","# Ppi = np.zeros([2,2])\n","# for s in states:\n","#   for s_next in states:\n","#     Ppi[s, s_next] = np.sum(pi[s] * P[s, :, s_next]) \n","\n","# Compute the dynamics given the policy\n","Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n","Rpi = np.sum(R * pi, axis=1) #Note, we assume that R(s,a) does not depend of the next state\n","\n","# Evaluate the policy\n","Vpi = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n","print(\"## Vpi: \")\n","print(Vpi)\n","\n","\n","# # Compute the Q values\n","Qpi = R + gamma * P.dot(Vpi)\n","\n","# print(\"## Qpi:\")\n","print(\"## Qpi: \")\n","print(Qpi)\n","\n","\n","# # What is the next policy if we perform one step of policy improvment ?\n","dpi_new = Qpi.argmax(axis=1)\n","\n","# print(\"## new pi:\")\n","print(\"## pi: \")\n","print(sparsify_policy(dpi_new, Na=env.Na))\n","print(env.render_policy(dpi_new))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZb0Piis2YX4","colab_type":"code","colab":{}},"source":["# Policy evaluation (recursive)\n","\n","# Retrieve the environment MDP\n","P = env.P\n","R = env.R\n","gamma = env.gamma\n","\n","# Policy to evaluate\n","dpi = np.array([1, 0])\n","pi = sparsify_policy(dpi, Na=env.Na)\n","\n","# Compute the dynamics given the policy\n","Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n","Rpi = np.sum(R * pi, axis=1) \n","\n","# Stopping criterion -> maximum number of steps\n","epsilon = 1e-3\n","r_max = np.max(R)\n","max_k = int(np.log(r_max/epsilon)/np.log(1/gamma))+1\n","\n","v = np.array([-1, 1])\n","print(\"0:\", v)\n","for k in range(max_k):\n","  v = Rpi + gamma*Ppi.dot(v)\n","  print(\"{}:\".format(k), v)\n","\n","print()\n","\n","# Stopping criterion -> compute the infinite norm\n","v_new = np.array([-1, 1])\n","v_old = np.array([2*epsilon, 0])\n","k = 1\n","print(\"0:\", v_new)\n","while np.absolute(v_new-v_old).max() > epsilon:\n","  v_old = v_new\n","  v_new = Rpi + gamma*Ppi.dot(v_new)\n","  print(\"{}:\".format(k), v_new)\n","  k += 1\n","\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0nl8OfLwpdC","colab_type":"text"},"source":["## New Environment!!!"]},{"cell_type":"code","metadata":{"id":"qDjMyANRs8Z6","colab_type":"code","colab":{}},"source":["# New Maze environment\n","# s: start\n","# g: goal\n","# x: negative reward state\n","\n","grid1 = [\n","    ['', '', '', 'g'],\n","    ['', 'x', '', ''],\n","    ['s', '', '', '']\n","]\n","grid1_MAP = [\n","    \"+-------+\",\n","    \"| : : :G|\",\n","    \"| :x: : |\",\n","    \"|S: : : |\",\n","    \"+-------+\",\n","]\n","\n","env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRUnx8EstjFY","colab_type":"code","colab":{}},"source":["# Useful attributes\n","print(\"Set of states:\", env.states)\n","print(\"Set of actions:\", env.actions)\n","print(\"Number of states: \", env.Ns)\n","print(\"Number of actions: \", env.Na)\n","print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n","print(\"R has shape: \", env.R.shape)  \n","print(\"discount factor: \", env.gamma)\n","print(\"\")\n","\n","# Usefult methods\n","state = env.reset() # get initial state\n","print(\"initial state: \", state)\n","print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n","print(\"\")\n","\n","# A random policy\n","dpi = np.random.randint(env.Na, size = (env.Ns,))\n","print(\"random policy = \", dpi)\n","\n","# Interacting with the environment\n","print(\"(s, a, s', r):\")\n","for time in range(4):\n","    action = dpi[state]\n","    next_state, reward, done, info = env.step(action)\n","    print(state, action, next_state, reward) \n","    if done:\n","        break\n","    state = next_state\n","print(\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gsXJD1qF74Iy","colab_type":"code","colab":{}},"source":["dpi = np.random.randint(env.Na, size = (env.Ns,))\n","env.render_policy(dpi)\n","\n","state = env.reset()\n","env.render()\n","for i in range(5):\n","    action = dpi[state]\n","    state, reward, done, _ = env.step(action)\n","    env.render()\n","    if done:\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ugaKsHsOX5G","colab_type":"text"},"source":["## Exercice: Value Iteration\n","1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n","2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition \n","3. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n","4. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"]},{"cell_type":"code","metadata":{"id":"AvlitQzHD7Fc","colab_type":"code","colab":{}},"source":["# useful function\n","def plot_infnorm(lst, star, name=\"V\"):\n","  \n","  lst = np.array(lst)\n","  star = np.array(star)\n","\n","  # Compute inf norm\n","  diff = np.absolute(lst - star).max(axis=1)\n","  plt.figure()\n","  plt.plot(diff)\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Error')\n","  plt.title(\"||{} - {}*||_inf\".format(name, name))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKkDqSyBOX5G","colab_type":"code","colab":{}},"source":["# Compute Value Iteration\n","\n","# Retrieve the environment MDP\n","P = env.P\n","R = env.R\n","gamma = env.gamma\n","epsilon = 1e-3\n","\n","# Prepare v, and storage\n","v = np.zeros(env.Ns)\n","v_all = []\n","pi_all = []\n","\n","# iterate over the value\n","while True:\n","  \n","  # Compute v_k\n","  v, v_old = np.max(R + gamma * P.dot(v), axis=1), v\n","  v_all.append(v)\n","\n","  # Esimate Intermediate policy\n","  q = R + gamma * P.dot(v)\n","  dpi = q.argmax(axis=1)\n","\n","  pi_all.append(dpi)\n","\n","  # stopping criterion \n","  if np.absolute(v-v_old).max() < epsilon: \n","    break\n","\n","# Plot optimal policy\n","env.render_policy(dpi)\n","\n","# You need to evaluate V* here from your optimal policy\n","# You have no guarantee that the last v-value is converge to the actual Vpi\n","pi = sparsify_policy(dpi, Na=env.Na)\n","Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n","Rpi = np.sum(R * pi, axis=1)\n","\n","v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n","print(\"v (last):\", v_all[-1])\n","print(\"v (star):\", v_star)\n","\n","# The difference is not exactly epsilon. We here we are comparing V*-V while the algorithm we are comparing V_new - V_old\n","print(\"epsilon: \", epsilon)\n","print(\"|v - v_star|_inf: \", np.absolute(v_all[-1]-v_star).max())\n","\n","# The value function converge towards v* iteration after iteration\n","plot_infnorm(v_all, v_star, name=\"V\")\n","\n","# If we estimate the intermediate policies, we observe that the optimal policy is obtained earlier in the process.\n","# However, the value function has not yet converge. Thus ares still unaware that it is the optimal policty at iteration 4.\n","plot_infnorm(pi_all, dpi, name=\"Pi\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDT4iyJnAPIE","colab_type":"text"},"source":["## Exercice: Policy Iteration (Homework)\n","1. Implement Policy iteration!\n","2. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n","3. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"]},{"cell_type":"code","metadata":{"id":"fpBCGnqNE7l1","colab_type":"code","colab":{}},"source":["# Retrieve the environment MDP\n","P = env.P\n","R = env.R\n","gamma = env.gamma\n","epsilon = 5e-2\n","\n","#Initialization of the policy\n","dpi = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n","print(env.render_policy(dpi))\n","id_max = np.identity(env.Ns)\n","Vpi = np.zeros(env.Ns)\n","sum_pi = []\n","sum_Vpi = []\n","while True:\n"," #Evaluation of the policy\n","  sum_Vpi.append(Vpi)\n","  Vpi = (R + gamma * np.dot(P, Vpi)).max(axis=1)\n","  sum_pi.append(dpi)\n","  q = R + gamma * P.dot(Vpi)\n","  dpi = q.argmax(axis=1)\n","\n","  \n","  \n","  if np.abs(sum_pi[-1] -  dpi).max() <= epsilon: break\n","\n","print(env.render_policy(dpi))\n","plot_infnorm(sum_pi, dpi, name=\"Pi\")\n","plot_infnorm(sum_Vpi, Vpi, name=\"Vpi\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHrlauOwOX5L","colab_type":"text"},"source":["## Exercice: Q learning\n","Q learning is a model-free algorithm for estimating the optimal Q-function online.\n","It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n","\n","1. Implement Q learning with $\\epsilon$-greedy exploration.\n","  - Plot the error in Q-functions over iterations\n","  - Plot the cumulative sum of rewards"]},{"cell_type":"code","metadata":{"id":"QCxJruBNOX5M","colab_type":"code","colab":{}},"source":["# ---------------------------\n","# Q-Learning\n","# ---------------------------\n","class QLearning:\n","    \"\"\"\n","    Q learning with epsilon-greedy exploration\n","    \"\"\"\n","    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n","        self.env = env\n","        self.gamma = gamma\n","        self.learning_rate = learning_rate\n","        self.epsilon = epsilon\n","        self.min_epsilon = min_epsilon\n","        self.Q = np.zeros((env.Ns, env.Na))\n","        self.Nsa = np.zeros((env.Ns, env.Na))\n","    \n","    def sample_action(self, state, greedy=False):\n","        epsilon = self.epsilon / max(1, math.sqrt(self.Nsa[state].min()))\n","        \n","        # explore\n","        if not greedy and np.random.uniform(0, 1) < epsilon:\n","          return np.random.choice(self.env.actions)\n","\n","        # exploit\n","        else:\n","          action = self.Q[state, :].argmax()\n","          return self.env.actions[action]\n","        \n","    \n","    def update(self, state, action, next_state, reward, done):\n","        alpha = self.learning_rate / max(1, math.sqrt(self.Nsa[state, action]))\n","        \n","        if not done:\n","          max_q = self.Q[next_state, :].max()\n","        else:\n","          max_q = 0.  # We do not bootstrap further\n","        \n","        q = self.Q[state, action]\n","        increment = (reward + self.gamma*max_q) - q\n","\n","        # Update\n","        self.Q[state, action] = self.Q[state, action] + alpha*increment\n","        self.Nsa[state, action] += 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsmojgubLVL9","colab_type":"code","colab":{}},"source":["q_learning = QLearning(env, gamma=env.gamma, learning_rate=1, epsilon=0.6, min_epsilon=0.1)\n","\n","# Define storage and variable\n","q_all = []\n","r_all = []\n","pi_all = []\n","max_steps = int(5e4)\n","\n","\n","# main algorithmic loop\n","state = env.reset()\n","for t in range(max_steps):\n","    \n","  # Sample the action\n","  action = q_learning.sample_action(state, greedy=False)\n","  \n","  # Sample the environment\n","  next_state, reward, done, _ = env.step(action)\n","  \n","  # Update q-function\n","  q_learning.update(state=state, action=action, next_state=next_state, reward=reward, done=done)\n","\n","  # Store information \n","  r_all.append(reward)\n","  q_all.append(q_learning.Q)\n","  pi_all.append(q_learning.Q.argmax(axis=1))\n","  \n","  state = next_state\n","  if done:\n","    state = env.reset()\n","\n","dpi = q_learning.Q.argmax(axis=1)\n","print(env.render_policy(dpi))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysm4Cq_pxj5T","colab_type":"code","colab":{}},"source":["state = env.reset()\n","env.render()\n","for i in range(5):\n","    action = q_learning.sample_action(state, greedy=True)\n","    next_state, reward, done, _ = env.step(action)\n","    state, reward, done, _ = env.step(action)\n","    env.render()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QyNzQpLIpAv","colab_type":"code","colab":{}},"source":["# Does the Q-value in the goal-state make sense?\n","print(q_learning.Q)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bi2j8vI4It8h","colab_type":"code","colab":{}},"source":["plot_infnorm(pi_all, pi_all[-1], name=\"Pi\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xViQ6kpzJfw9","colab_type":"code","colab":{}},"source":["pi = sparsify_policy(pi_all[-1], Na=env.Na)\n","Ppi = np.sum(env.P * np.expand_dims(pi, axis=-1), axis=1)\n","Rpi = np.sum(env.R * pi, axis=1)\n","\n","v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n","\n","v_all = []\n","for dpi_prev, q_prev in zip(pi_all, q_all):\n","  pi_prev = sparsify_policy(dpi_prev, Na=env.Na)\n","  v_prev = np.sum(pi_prev*q_prev, axis=1)\n","  v_all.append(v_prev)\n","\n","\n","# You may notive that some states have a high errors (especially the one with the negative rewards)\n","# Indeed, as there are less and less explored, the Q-values in \"bad\" states are not updated anymore as they are avoided by the agent.\n","plot_infnorm(v_all, v_star, name=\"V\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59xsIjpIOX5P","colab_type":"text"},"source":["## Exercice: SARSA (Homework)\n","SARSA is another control algorithm. While Qlearning is off-policy, SARSA is on-policy.\n","\n","1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n","2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$"]},{"cell_type":"code","metadata":{"id":"gySnHtrsOX5Q","colab_type":"code","colab":{}},"source":["# ---------------------------\n","# SARSA\n","# ---------------------------\n","class SARSA:\n","    \"\"\"\n","    SARSA with deacreasing epsilon for exploration\n","    \"\"\"\n","    def __init__(self, env, gamma, learning_rate, epsilon):\n","      # Start with a random policy\n","\n","        self.env = env\n","        self.gamma = gamma\n","        self.learning_rate = learning_rate\n","        self.epsilon = epsilon\n","        self.q = np.zeros((env.Ns, env.Na))\n","        self.Nsa = np.zeros((env.Ns, env.Na))\n","\n","    def sample_action(self, state, greedy=False):\n","      epsilon = self.epsilon / (math.sqrt(self.Nsa[state].min()) + 1)\n","\n","\n","      if not greedy and np.random.uniform(0, 1) < epsilon:\n","        return np.random.choice(self.env.actions)\n","      else:\n","        return self.env.actions[self.q[state].argmax()]\n","    \n","    def update(self, state, action, next_state, next_action, reward):\n","      self.Nsa[state][action] += 1\n","\n","      alp_ = self.learning_rate / math.sqrt(self.Nsa[state, action])\n","      self.q[state][action] +=  alp_ * (reward + (self.gamma * self.q[next_state][next_action]) - self.q[state][action])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eK05C1Laotw5","colab_type":"code","colab":{}},"source":["sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n","\n","# Define storage and variable\n","q_all = []\n","r_all = []\n","pi_all = []\n","max_steps = int(5e4)\n","\n","\n","# main algorithmic loop\n","state = env.reset()\n","for t in range(max_steps):\n","    \n","  # Sample the action\n","  action = sarsa.sample_action(state)\n","  \n","  # Sample the environment\n","  next_state, reward, done, info = env.step(action)\n","  next_action = sarsa.sample_action(next_state)\n","  \n","  # Update q-function\n","  sarsa.update(state, action, next_state, next_action, reward)\n","\n","  # Store information \n","  r_all.append(reward)\n","  q_all.append(sarsa.q.copy())\n","  pi_all.append(sarsa.q.argmax(axis=1))\n","  \n","  state = next_state\n","  if done:\n","    state = env.reset()\n","\n","dpi = sarsa.q.argmax(axis=1)\n","print(env.render_policy(dpi))\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be0d4nOBOX5R","colab_type":"code","colab":{}},"source":["\n","sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n","\n","...\n","\n","\n","\n","sum_q = []\n","sum_r = []\n","sum_pi = []\n","max_steps = int(5e4)\n","State = env.reset()\n","for t in range(max_steps):\n","  action = sarsa.sample_action(State)\n","  next_state, reward, done, info = env.step(action)\n","  next_action = sarsa.sample_action(next_state)\n","  sarsa.update(State, action, next_state, next_action, reward)\n","  sum_r.append(reward)\n","  sum_q.append(sarsa.q.copy())\n","  sum_pi.append(sarsa.q.argmax(axis=1))\n","  \n","  State = next_state\n","  if done:\n","    State = env.reset()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytPcSPVxsBNR","colab_type":"code","colab":{}},"source":["State = env.reset()\n","env.render()\n","for i in range(5):\n","    action = sarsa.sample_action(State, greedy=True)\n","    State, reward, done, _ = env.step(action)\n","    env.render()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xho5u0y4sCXF","colab_type":"code","colab":{}},"source":["plot_infnorm(sum_q, sarsa.q, name=\"Sarsa\")\n","plot_infnorm(sum_pi, dpi, name=\"Pi\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RG4FoVjtsvsg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}