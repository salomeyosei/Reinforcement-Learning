{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Copy of Practical Session DQN.ipynb","provenance":[{"file_id":"1Mtv6T2rgk6y0-WLqkYt0PYxzpOsKh-40","timestamp":1582554546552},{"file_id":"https://github.com/rlgammazero/mvarl_hands_on/blob/master/dqn/DQN.ipynb","timestamp":1582552813048}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"opZzyYN7fwxD","colab_type":"text"},"source":["## Colab setup"]},{"cell_type":"code","metadata":{"id":"rbV50a2jfwxJ","colab_type":"code","colab":{}},"source":["!pip install gym > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nA_0eutEfwxS","colab_type":"code","colab":{}},"source":["\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqhcwbE3fwxX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b03cca4d-5689-44cf-da1a-e5b154c4b553","executionInfo":{"status":"ok","timestamp":1582554712961,"user_tz":0,"elapsed":14815,"user":{"displayName":"Salomey Osei","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwKQNJKkGDsawNh2uJOFgmzdXst-BfL5RYMLRI=s64","userId":"00979995660498364528"}}},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (45.2.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FFQNOxGsfwxc","colab_type":"text"},"source":["# Deep Q-Learning (DQN)\n","\n","\n","In DQN, the $Q$-function is parameterized by a neural network of parameters $\\theta$. The network takes as input a state $s$ and outputs $Q(s, a, \\theta)$ for all actions $a$. \n","\n","The network is trained in way that is similar to Fitted Q Iteration. At each time $T$, the agent has observed the transitions $(s_t, a_t, r_t, s_t')_{t=1}^T$, which are stored in a __replay buffer__.\n","\n","In addition to the network with parameters $\\theta$, DQN keeps another network with the same architecture and parameters $\\tilde{\\theta}$, called __target network__. \n","To update the parameters $\\theta$, we sample $N$ transitions from the __replay buffer__, we define the loss \n","\n","$$\n","L(\\theta) = \\sum_{i=1}^N [Q(s_i, a_i, \\theta) - (r_i + \\gamma\\max_{a'}Q(s'_i,a', \\tilde{\\theta}))]^2\n","$$\n","\n","and update \n","\n","$$\n","\\theta \\gets \\theta + \\eta \\nabla L(\\theta).\n","$$\n","\n","\n","Every $C$ iterations, the target network is updated as $\\tilde{\\theta} \\gets \\theta$. \n","\n","At each time $t$, DQN updates the networks as described above, selects an action according to an $\\epsilon$-greedy policy, plays the action and stores the new data in the replay buffer."]},{"cell_type":"code","metadata":{"id":"RrBtkvHqfwxe","colab_type":"code","colab":{}},"source":["# Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","from copy import deepcopy\n","\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","from IPython.display import clear_output\n","from pathlib import Path\n","\n","import random, os.path, math, glob, csv, base64, itertools, sys\n","from pprint import pprint\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import io\n","from IPython.display import HTML"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rz3beIAHfwxi","colab_type":"text"},"source":["## Step 1: Define the parameters"]},{"cell_type":"code","metadata":{"id":"SV7lKaVGfwxj","colab_type":"code","colab":{}},"source":["# Environment\n","env = gym.make(\"CartPole-v0\")\n","\n","# Discount factor\n","GAMMA = 0.99\n","\n","# Batch size\n","BATCH_SIZE = 256\n","# Capacity of the replay buffer\n","BUFFER_CAPACITY = 10000\n","# Update target net every ... episodes\n","UPDATE_TARGET_EVERY = 20\n","\n","# Initial value of epsilon\n","EPSILON_START = 1.0\n","# Parameter to decrease epsilon\n","DECREASE_EPSILON = 200\n","# Minimum value of epislon\n","EPSILON_MIN = 0.05\n","\n","# Number of training episodes\n","N_EPISODES = 200\n","\n","# Learning rate\n","LEARNING_RATE = 0.1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4fVXZSafwxo","colab_type":"text"},"source":["## Step 2: Define the replay buffer"]},{"cell_type":"code","metadata":{"id":"aH1tb1wIfwxp","colab_type":"code","colab":{}},"source":["class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, state, action, reward, next_state):\n","        \"\"\"Saves a transition.\"\"\"\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = (state, action, reward, next_state)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFgVBS8Gfwxt","colab_type":"code","colab":{}},"source":["# create instance of replay buffer\n","replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6ki1HpXfwxw","colab_type":"text"},"source":["## Step 3: Define the neural network architecture, objective and optimizer"]},{"cell_type":"code","metadata":{"id":"pP2R3fZ8fwxy","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    \"\"\"\n","    Basic neural net.\n","    \"\"\"\n","    def __init__(self, obs_size, hidden_size, n_actions):\n","        super(Net, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(obs_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, n_actions)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_Oz9uvzfwx2","colab_type":"code","colab":{}},"source":["# create network and target network\n","hidden_size = 128\n","obs_size = env.observation_space.shape[0]\n","n_actions = env.action_space.n\n","\n","q_net = Net(obs_size, hidden_size, n_actions)\n","target_net = Net(obs_size, hidden_size, n_actions)\n","\n","# objective and optimizer\n","objective = nn.MSELoss()\n","optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYeNGkE0fwx5","colab_type":"text"},"source":["## Step 4: Implement DQN"]},{"cell_type":"code","metadata":{"id":"EQqeJNoOfwx7","colab_type":"code","colab":{}},"source":["#\n","#  Some useful functions\n","#\n","\n","def get_q(states):\n","    \"\"\"\n","    Compute Q function for a list of states\n","    \"\"\"\n","    with torch.no_grad():\n","        states_v = torch.FloatTensor([states])\n","        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n","    return output[0, :, :]  # shape (len(states), n_actions)\n","\n","def eval_dqn(n_sim=5):\n","    \"\"\"\n","    Monte Carlo evaluation of DQN agent\n","    \"\"\"\n","    rewards = np.zeros(n_sim)\n","    return rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvgkbH0Vfwx-","colab_type":"code","colab":{}},"source":["def choose_action(state, epsilon):\n","  q_state= get_q([state])[0] #array of shape (n_actions,)\n","  if np.random.uniform(0,1) < epsilon:\n","    action = env.action_space.sample() #random action\n","  else:\n","    action = q_state.argmax()\n","  return action\n","    \"\"\"\n","    TO BE IMPLEMENTED\n","    \n","    Return action according to an epsilon-greedy exploration policy\n","    \"\"\"\n","    return 0\n","    \n","\n","def update(state, action, reward, next_state, done):\n","\n","    \"\"\"\n","    TO BE COMPLETED\n","    \"\"\"\n","    \n","    # add data to replay buffer\n","    if done:\n","        next_state = None\n","    replay_buffer.push(state, action, reward, next_state)\n","    \n","    if len(replay_buffer) < BATCH_SIZE:\n","        return np.inf\n","    \n","    # get batch\n","    transitions = replay_buffer.sample(BATCH_SIZE)\n","    #first thing: compute Q(s_I, a_i,teta for all (s_I, a_i,))\n","    #in the batch\n","\n","    #build tensor with s_i and tensor with a_i\n","\n","batch_states= torch.FloatTensor(\n","    [transitions[ii][0] for ii in range(BATCH_SIZE)]\n","     )\n","batch_actions= torch.FloatTensor(\n","    [transitions[ii][0] for ii in range(BATCH_SIZE)]\n","     )\n","batch_states= torch.FloatTensor(\n","    [transitions[ii][0] for ii in range(BATCH_SIZE)]\n","     )\n","    \n","    # Compute loss - TO BE IMPLEMENTED!\n","    values  = torch.zeros(BATCH_SIZE)   # to be computed using batch\n","    targets = torch.zeros(BATCH_SIZE)   # to be computed using batch\n","    loss = objective(values, targets)\n","     \n","    # Optimize the model - UNCOMMENT!\n","#     optimizer.zero_grad()\n","#     loss.backward()\n","#     optimizer.step()\n","    \n","    return loss.data.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUzuCwdcfwyC","colab_type":"code","outputId":"6c6b2280-213f-4cd2-cb76-bfe351103703","colab":{}},"source":["#\n","# Train\n","# \n","\n","EVAL_EVERY = 5\n","REWARD_THRESHOLD = 199\n","\n","def train():\n","    state = env.reset()\n","    epsilon = EPSILON_START\n","    ep = 0\n","    total_time = 0\n","    while ep < N_EPISODES:\n","        action = choose_action(state, epsilon)\n","\n","        # take action and update replay buffer and networks\n","        next_state, reward, done, _ = env.step(action)\n","        loss = update(state, action, reward, next_state, done)\n","\n","        # update state\n","        state = next_state\n","\n","        # end episode if done\n","        if done:\n","            state = env.reset()\n","            ep   += 1\n","            if ( (ep+1)% EVAL_EVERY == 0):\n","                rewards = eval_dqn()\n","                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n","                if np.mean(rewards) >= REWARD_THRESHOLD:\n","                    break\n","\n","            # update target network\n","            if ep % UPDATE_TARGET_EVERY == 0:\n","                target_net.load_state_dict(q_net.state_dict())\n","            # decrease epsilon\n","            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n","                            np.exp(-1. * ep / DECREASE_EPSILON )    \n","\n","        total_time += 1\n","\n","train()\n","rewards = eval_dqn(20)\n","print(\"\")\n","print(\"mean reward after training = \", np.mean(rewards))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["episode = 5 , reward =  0.0\n","episode = 10 , reward =  0.0\n","episode = 15 , reward =  0.0\n","episode = 20 , reward =  0.0\n","episode = 25 , reward =  0.0\n","episode = 30 , reward =  0.0\n","episode = 35 , reward =  0.0\n","episode = 40 , reward =  0.0\n","episode = 45 , reward =  0.0\n","episode = 50 , reward =  0.0\n","episode = 55 , reward =  0.0\n","episode = 60 , reward =  0.0\n","episode = 65 , reward =  0.0\n","episode = 70 , reward =  0.0\n","episode = 75 , reward =  0.0\n","episode = 80 , reward =  0.0\n","episode = 85 , reward =  0.0\n","episode = 90 , reward =  0.0\n","episode = 95 , reward =  0.0\n","episode = 100 , reward =  0.0\n","episode = 105 , reward =  0.0\n","episode = 110 , reward =  0.0\n","episode = 115 , reward =  0.0\n","episode = 120 , reward =  0.0\n","episode = 125 , reward =  0.0\n","episode = 130 , reward =  0.0\n","episode = 135 , reward =  0.0\n","episode = 140 , reward =  0.0\n","episode = 145 , reward =  0.0\n","episode = 150 , reward =  0.0\n","episode = 155 , reward =  0.0\n","episode = 160 , reward =  0.0\n","episode = 165 , reward =  0.0\n","episode = 170 , reward =  0.0\n","episode = 175 , reward =  0.0\n","episode = 180 , reward =  0.0\n","episode = 185 , reward =  0.0\n","episode = 190 , reward =  0.0\n","episode = 195 , reward =  0.0\n","episode = 200 , reward =  0.0\n","\n","mean reward after training =  0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nhWv6ThkfwyG","colab_type":"text"},"source":["## Visualizing the agent"]},{"cell_type":"code","metadata":{"id":"n0sGUpzJfwyH","colab_type":"code","outputId":"a73e58bd-17a8-4f8a-dc85-c6cadac2c7b7","colab":{}},"source":["def show_video(directory):\n","    html = []\n","    for mp4 in Path(directory).glob(\"*.mp4\"):\n","        video_b64 = base64.b64encode(mp4.read_bytes())\n","        html.append('''<video alt=\"{}\" autoplay \n","                      loop controls style=\"height: 400px;\">\n","                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                 </video>'''.format(mp4, video_b64.decode('ascii')))\n","    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n","    \n","def make_seed(seed):\n","    np.random.seed(seed=seed)\n","    torch.manual_seed(seed=seed)\n","  \n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1028'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1028'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"b2Fhe2iefwyM","colab_type":"code","outputId":"372cdc45-ca58-48ea-c666-2a108df0b087","colab":{}},"source":["env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n","for episode in range(1):\n","    done = False\n","    state = env.reset()\n","    while not done:\n","        action = env.action_space.sample() # MODIFY THIS PART TO COMPUTE THE ACTION WITH DQN\n","        state, reward, done, info = env.step(action)\n","env.close()\n","show_video(\"./gym-results\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<video alt=\"gym-results/openaigym.video.0.3274.video000000.mp4\" autoplay \n","                      loop controls style=\"height: 400px;\">\n","                      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","                 </video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]}]}