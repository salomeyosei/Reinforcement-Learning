{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"Copy of AMMI_EXPRL_Exercise.ipynb","provenance":[{"file_id":"1Xz6-c4-b1i0N5QtbdshGnopB1AfPR4cv","timestamp":1582726118819}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_UWM-gGi76pF","colab_type":"code","colab":{}},"source":["!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5EbzJ1A78Bk","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.insert(0, './mvarl_hands_on/exploration')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDPYhYvR7yKZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","from riverswim import RiverSwim\n","import cvxpy as cp\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5twn7Qv7yKf","colab_type":"text"},"source":["# Finite-Horizon MDPs\n","We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n","\n","The value of a policy or the optimal value function can be computed using *backward induction*.\n","\n","\n","Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n","$$\n","V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n","$$\n","where $V_{H+1}(s) = 0$, for any $s$. "]},{"cell_type":"code","metadata":{"id":"7l8433mz7yKh","colab_type":"code","outputId":"b816988d-4f14-4c0a-ee6d-92c7e3652614","executionInfo":{"status":"ok","timestamp":1582726763189,"user_tz":0,"elapsed":813,"user":{"displayName":"Salomey Osei","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwKQNJKkGDsawNh2uJOFgmzdXst-BfL5RYMLRI=s64","userId":"00979995660498364528"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["env = RiverSwim(6)\n","H = 10\n","print(\"Reward matrix: \", env.R.shape)\n","print(env.R)\n","print()\n","print(\"Transition matrix: \", env.P.shape)\n","print(\"Transitions probabilities for state s_1:\")\n","print(env.P[1])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Reward matrix:  (6, 2)\n","[[0.005 0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    1.   ]]\n","\n","Transition matrix:  (6, 2, 6)\n","Transitions probabilities for state s_1:\n","[[1.   0.   0.   0.   0.   0.  ]\n"," [0.05 0.6  0.35 0.   0.   0.  ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hiXHdYle7yKm","colab_type":"text"},"source":["# Backward induction (aka Value Iteration)"]},{"cell_type":"code","metadata":{"id":"Q_KZXn8A7yKo","colab_type":"code","colab":{}},"source":["def backward_induction(P, R, H):\n","    \"\"\"\n","        Parameters:\n","            P: transition function (S,A,S)-dim matrix\n","            R: reward function (S,A)-dim matrix\n","            H: horizon\n","\n","        Returns:\n","            The optimal V-function: array of shape (horizon)\n","            The optimal policy\n","    \"\"\"\n","    S, A = P.shape[0], P.shape[1]\n","    policy = np.zeros((H, S), dtype=np.int)\n","    V = np.zeros((H + 1, S))\n","    for h in reversed(range(H)):\n","        for s in range(S):\n","            \"\"\" \n","            Here, we compute V^*(h, s) using the Bellman optimality equation:\n","\n","            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n","            \"\"\"\n","            for a in range(A):\n","                tmp = np.dot(P[s, a], R[s, a] + V[h + 1])\n","                if (a == 0) or (tmp > V[h, s]):\n","                    policy[h, s] = a\n","                    V[h, s] = tmp\n","    return V, policy\n","\n","def policy_evaluation(P, R, H, policy):\n","    \"\"\"\n","        Parameters:\n","            P: transition function (S,A,S)-dim matrix\n","            R: reward function (S,A)-dim matrix\n","            H: horizon\n","            policy: policy (H,S)-dim matrix\n","\n","        Returns:\n","            The V-function of the given policy\n","    \"\"\"\n","    S, A = P.shape[0], P.shape[1]\n","    V = np.zeros((H + 1, S))\n","    for h in reversed(range(H)):\n","        for s in range(S):\n","            \"\"\" \n","            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n","\n","            a = policy[h,s]\n","            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n","            \"\"\"\n","            a = policy[h,s]\n","            # complete the policy evalution here\n","            V[h,s] = R[s,a] + (P[s,a, :]).dot(V[h+1,  :])\n","    return V"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvmhRCN07yKs","colab_type":"text"},"source":["Compute solution"]},{"cell_type":"code","metadata":{"id":"vVjkydge7yKt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":880},"outputId":"680ae5de-75d8-42ad-8e9d-50703f94180f","executionInfo":{"status":"ok","timestamp":1582727982390,"user_tz":0,"elapsed":912,"user":{"displayName":"Salomey Osei","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwKQNJKkGDsawNh2uJOFgmzdXst-BfL5RYMLRI=s64","userId":"00979995660498364528"}}},"source":["Vstar, POLstar = backward_induction(env.P, env.R, H)\n","\n","print(\"Optimal policy\")\n","print(np.round(Vstar))\n","\n","print(POLstar)\n","print(\"difference between v_policy - vstar\")\n","V_policy=policy_evaluation(env.P, env.R, H, POLstar)\n","print(np.abs(V_policy -Vstar).sum())\n","print(\".........\")\n","print(V_policy)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Optimal policy\n","[[0. 1. 1. 2. 4. 5.]\n"," [0. 0. 1. 2. 3. 5.]\n"," [0. 0. 1. 2. 3. 4.]\n"," [0. 0. 1. 1. 3. 4.]\n"," [0. 0. 0. 1. 2. 3.]\n"," [0. 0. 0. 1. 2. 3.]\n"," [0. 0. 0. 0. 1. 3.]\n"," [0. 0. 0. 0. 1. 2.]\n"," [0. 0. 0. 0. 0. 2.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0.]]\n","[[1 1 1 1 1 1]\n"," [1 1 1 1 1 1]\n"," [1 1 1 1 1 1]\n"," [1 1 1 1 1 1]\n"," [0 1 1 1 1 1]\n"," [0 1 1 1 1 1]\n"," [0 0 1 1 1 1]\n"," [0 0 0 1 1 1]\n"," [0 0 0 0 1 1]\n"," [0 0 0 0 0 1]]\n","difference between v_policy - vstar\n","7.16093850883226e-15\n",".........\n","[[3.52383978e-01 6.55026995e-01 1.39695137e+00 2.48242727e+00\n","  3.79959810e+00 5.21762699e+00]\n"," [2.11497901e-01 4.46308029e-01 1.07619224e+00 2.08263037e+00\n","  3.36868410e+00 4.78358892e+00]\n"," [1.12394222e-01 2.77567020e-01 7.83280303e-01 1.69241630e+00\n","  2.93719022e+00 4.34785472e+00]\n"," [5.34228750e-02 1.51708453e-01 5.25345156e-01 1.31567939e+00\n","  2.50497544e+00 3.90977425e+00]\n"," [3.00000000e-02 6.90381250e-02 3.10815937e-01 9.58296250e-01\n","  2.07188812e+00 3.46836500e+00]\n"," [2.50000000e-02 2.62312500e-02 1.48712500e-01 6.29362500e-01\n","  1.63783750e+00 3.02205000e+00]\n"," [2.00000000e-02 1.50000000e-02 4.63750000e-02 3.43250000e-01\n","  1.20312500e+00 2.56800000e+00]\n"," [1.50000000e-02 1.00000000e-02 5.00000000e-03 1.22500000e-01\n","  7.70000000e-01 2.10000000e+00]\n"," [1.00000000e-02 5.00000000e-03 0.00000000e+00 0.00000000e+00\n","  3.50000000e-01 1.60000000e+00]\n"," [5.00000000e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 1.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SZYYngKs8Z-8","colab_type":"code","colab":{}},"source":["S,A = env.R.shape #number of states and action\n","Phat = np.zeros(S,A, S)\n","Rhat = np.zeros(S,A, S)\n","\n","N_sa=np.zzeros(S,A)\n","N_sas =  np.zeros(S,A, S)\n","S_sa =  np.zeros(S,A)\n","\n","nb_episodes = 10\n","#loop over episodes\n","for ep in range(episodes):\n","  state=env.reset()\n","  for h in range(H):\n","    action = np.random.choice(A)\n","    next_state,reward,done,_ = env.step(action)\n","\n","    #update estimates\n","    N_sa[state,action] += 1\n","    N_sas[state,action], nextstate += 1\n","    S_sa[state,action] += reward\n","\n","    Rhat[state, action] = S_sa[state,action]/N_sa(N_sta)\n","print (Rhat)\n","print(Phat)\n","print(env.R)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1nCGWuw7yKy","colab_type":"text"},"source":["## UCRL"]},{"cell_type":"markdown","metadata":{"id":"zoWtsh7v7yK0","colab_type":"text"},"source":["UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n","In this setting, the regret is defined as\n","$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n","UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n","\n","The structure of the algorithm is as follow\n","\n","For $k = 1, \\ldots, K$ do<br>\n","> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n","> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",">> for $h=1, \\ldots, H$<br>\n",">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n","\n","<font color='#ed7d31'>Optimistic planning</font>\n","At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n","$$\n","V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n","$$\n","where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n","\n","$$\n","B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n","$$\n","\n","$$\n","B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n","$$\n","where \n","\n","$$\n","\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n","\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n","$$\n","and where  $N^+(s, a) = \\max(1, N(s, a))$."]},{"cell_type":"markdown","metadata":{"id":"ji7IShLN7yK1","colab_type":"text"},"source":["---\n","The following function computes:\n","$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n","where $B_p = [P-\\beta, P+\\beta]$"]},{"cell_type":"code","metadata":{"id":"57zVp32L7yK3","colab_type":"code","colab":{}},"source":["def LPprobaH(v, P, beta, verbose=0):\n","    \"\"\"\n","        max_x v^T x\n","        s.t.    0 <= x_i <= 1\n","                \\sum_i |x_i - p_i| \\leq beta\n","                \\sum_i x_i = 1\n","    \"\"\"\n","    sorted_idxs = np.argsort(v)[::-1]\n","\n","    pest = P.copy()\n","    idx = sorted_idxs[0]\n","    pest[idx] = min(1., P[idx] + beta / 2.)\n","    delta = pest.sum()\n","    j = len(P)-1\n","    while delta > 1:\n","        idx_j = sorted_idxs[j]\n","        m = max(0, 1. - delta + pest[idx_j])\n","        delta = delta - pest[idx_j] + m\n","        pest[idx_j] = m\n","        j -= 1\n","    w = np.dot(pest, v)\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pk0v7jg67yK7","colab_type":"code","colab":{}},"source":["def UCRL(mdp, H, nb_episodes, VSTAR=0):\n","    S, A = mdp.Ns, mdp.Na\n","    policy = np.zeros((H, S), dtype=np.int)\n","    Phat = np.ones((S,A,S)) / S\n","    Rhat = np.zeros((S,A))\n","    N_sas = np.zeros((S,A,S), dtype=np.int)\n","    N_sa = np.zeros((S,A), dtype=np.int)\n","    regret = np.zeros((nb_episodes,))\n","    V = np.zeros((H + 1, S))\n","    \n","    delta = 0.1\n","    \n","    for k in range(nb_episodes):\n","        \n","        # compute optimistic solution\n","        # 1. compute confidence intervals\n","        N = np.maximum(N_sa, 1)\n","        LOGT = np.log(S * A * N / delta)\n","        beta_r = ...\n","        beta_p = ...\n","        \n","        # 2. run extended value iteration\n","        V.fill(0)\n","        for h in reversed(range(H)):\n","            for s in range(S):\n","                for a in range(A):\n","                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n","                    ...\n","        \n","        # execute policy\n","        initial_state = state = mdp.reset()\n","        for h in range(H):\n","            action = policy[h][state]\n","            next_state, reward, done, _ = mdp.step(action)\n","            \n","            # update estimates (Phat, Rhat, N_sa, N_sas)\n","            ...\n","            \n","            state = next_state\n","        \n","        # update regret\n","        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n","        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n","        \n","        if k % 50 == 0:\n","            print(\"regret[{}]: {}\".format(k, regret[k]))\n","    return regret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-UTr3Gq7yK_","colab_type":"code","colab":{}},"source":["nb_repetitions = 5\n","nb_episodes = 750\n","regrets = np.zeros((nb_repetitions, nb_episodes))\n","for it in range(nb_repetitions):\n","    print(\"Running simulation: {}\".format(it))\n","    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M49yfZ_S7yLE","colab_type":"code","colab":{}},"source":["x = regrets.cumsum(axis=-1)\n","mean_regret = x.mean(axis=0)\n","std_regret = x.std(axis=0)\n","plt.plot(mean_regret)\n","plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n","plt.ylabel('regret')\n","\n","# SAVE PSRL REGRET\n","ucrl_regret = mean_regret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myPuG9Er7yLL","colab_type":"text"},"source":["# Posterior Sampling for RL\n","\n","At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."]},{"cell_type":"markdown","metadata":{"id":"9gxP6B9r7yLN","colab_type":"text"},"source":["Implement posterior sampling"]},{"cell_type":"code","metadata":{"id":"cl9YwtfJ7yLP","colab_type":"code","colab":{}},"source":["def PSRL(mdp, H, nb_episodes, VSTAR=0):\n","    reward_prior = [1,1]\n","    S, A = mdp.Ns, mdp.Na\n","    Phat = np.ones((S,A,S)) / S\n","    Rhat = np.zeros((S,A))\n","    N_sas = np.zeros((S,A,S), dtype=np.int)\n","    N_sa = np.zeros((S,A), dtype=np.int)\n","    regret = np.zeros((nb_episodes,))\n","    \n","    for k in range(nb_episodes):\n","        \n","        # compute policy\n","        # 1. sample MDP\n","        R = np.zeros_like(Rhat)\n","        P = np.zeros((S, A, S))\n","        for s in range(S):\n","            for a in range(A):\n","                # sample transition matrix\n","                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n","                P[s, a] = ...\n","\n","                # posterior for Bernoulli rewards\n","                N = N_sa[s, a]\n","                v = N * Rhat[s, a]\n","                a0 = reward_prior[0] + v\n","                b0 = reward_prior[1] + N - v\n","                p = np.random.beta(a=a0, b=b0, size=1).item()\n","                R[s, a] = p\n","        \n","        # 2. compute optimal policy\n","        V, policy = ...\n","        \n","        # execute policy\n","        initial_state = state = mdp.reset()\n","        for h in range(H):\n","            action = policy[h][state]\n","            next_state, reward, done, _ = mdp.step(action)\n","            \n","            # update estimates (Rhat, N_sa, N_sas)\n","            ...\n","            \n","            state = next_state\n","        \n","        # update regret\n","        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n","        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n","        \n","        if k % 50 == 0:\n","            print(\"regret[{}]: {}\".format(k, regret[k]))\n","\n","    return regret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT38aVOB7yLU","colab_type":"code","colab":{}},"source":["nb_repetitions = 5\n","nb_episodes = 750\n","regrets = np.zeros((nb_repetitions, nb_episodes))\n","for it in range(nb_repetitions):\n","    print(\"Running simulation: {}\".format(it))\n","    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQhP7cpn7yLY","colab_type":"code","colab":{}},"source":["x = regrets.cumsum(axis=-1)\n","mean_regret = x.mean(axis=0)\n","std_regret = x.std(axis=0)\n","plt.plot(mean_regret)\n","plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n","plt.ylabel('regret')\n","\n","# SAVE PSRL REGRET\n","psrl_regret = mean_regret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phHZWwdJ7yLc","colab_type":"text"},"source":["Compare algorithms"]},{"cell_type":"code","metadata":{"id":"cvcqwaJY7yLd","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10,8))\n","plt.plot(ucrl_regret, label='UCRL-H')\n","plt.plot(psrl_regret, label='PSRL')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"it2tQtxL7yLi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}